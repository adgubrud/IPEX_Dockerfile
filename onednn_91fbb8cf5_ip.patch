diff --git a/src/cpu/x64/jit_brgemm_inner_product.cpp b/src/cpu/x64/jit_brgemm_inner_product.cpp
index 56ca145..306bb73 100644
--- a/src/cpu/x64/jit_brgemm_inner_product.cpp
+++ b/src/cpu/x64/jit_brgemm_inner_product.cpp
@@ -70,8 +70,150 @@ void maybe_tile_configure(bool is_amx,
         amx_tile_configure(&brg_kernel_palettes[brg_ker_idx][0]);
     prev_ker_idx = brg_ker_idx;
 }
-} // namespace
 
+inline size_t get_dst_reduced_off(const jit_brgemm_primitive_conf_t &jbgp,
+        const memory_desc_wrapper &dst_d, int ithr_ic, int osb, int ocb) {
+    assert(jbgp.nthr_ic_b > 1);
+    int os = osb * jbgp.os_block;
+    int oc = ocb * jbgp.oc_block;
+    const size_t acc_dt_size = types::data_type_size(jbgp.acc_dt);
+    const size_t dst_off = get_blk_off(dst_d, jbgp.acc_dt, os, oc);
+    if (ithr_ic == 0) return dst_off;
+    assert(ithr_ic > 0);
+    const size_t ic_buf_idx = jbgp.with_sum || jbgp.acc_dt != jbgp.dst_dt
+            ? ithr_ic
+            : ithr_ic - 1;
+    return dst_off + (ic_buf_idx * jbgp.mb * jbgp.LDC * acc_dt_size);
+}
+
+inline bool init_thr_groups(const jit_brgemm_primitive_conf_t &jbgp,
+        const int ithr, const int nthr, int &nthr_ic, int &nthr_oc_mb,
+        int &ithr_ic, int &ithr_oc_mb) {
+    const int os_chunks = div_up(jbgp.nb_os, jbgp.nb_os_blocking);
+    const int oc_chunks = div_up(jbgp.nb_oc, jbgp.nb_oc_blocking);
+    const int ic_chunks = div_up(jbgp.nb_ic, jbgp.nb_ic_blocking);
+    const int work_amount = oc_chunks * os_chunks;
+
+    nthr_ic = jbgp.nthr_ic_b <= nthr ? jbgp.nthr_ic_b : 1;
+    nthr_oc_mb = nthr / nthr_ic;
+    ithr_ic = ithr / nthr_oc_mb;
+    ithr_oc_mb = ithr % nthr_oc_mb;
+    if (ithr_oc_mb >= work_amount || ithr_ic >= ic_chunks
+            || ithr >= rnd_dn(nthr, nthr_ic))
+        return false;
+    return true;
+}
+
+} // namespace
+template <cpu_isa_t isa>
+void brgemm_inner_product_fwd_t<isa>::maybe_reduce_and_apply_postops(
+        const jit_brgemm_primitive_conf_t &jbgp,
+        const memory_desc_wrapper &dst_d, int ithr, int nthr,
+        char *c_buffer_global, char *dst, bool are_post_ops_applicable,
+        const char *bias, const float *oscales, const float *dst_scales,
+        char *wsp_tile_base, const int32_t *compensation,
+        std::vector<const void *> post_ops_binary_rhs_arg_vec) const {
+    const int os_chunks = div_up(jbgp.nb_os, jbgp.nb_os_blocking);
+    const int oc_chunks = div_up(jbgp.nb_oc, jbgp.nb_oc_blocking);
+    const int work_amount = oc_chunks * os_chunks;
+    const bool is_amx = jbgp.is_amx;
+    const size_t bia_dt_size
+            = jbgp.with_bias ? types::data_type_size(jbgp.bia_dt) : 0;
+    const size_t acc_dt_size = types::data_type_size(jbgp.acc_dt);
+
+    int nthr_ic {1}, nthr_oc_mb {1}, ithr_ic {0}, ithr_oc_mb {0};
+    bool ok = init_thr_groups(
+            jbgp, ithr, nthr, nthr_ic, nthr_oc_mb, ithr_ic, ithr_oc_mb);
+    if (!ok) return;
+    int ocmb_start {0}, ocmb_end {0};
+    int start {0}, end {0};
+    balance211(work_amount, nthr_oc_mb, ithr_oc_mb, ocmb_start, ocmb_end);
+    balance211(ocmb_end - ocmb_start, nthr_ic, ithr_ic, start, end);
+
+    int prev_ker_idx = -1;
+
+    int occ {0}, osc {0};
+    nd_iterator_init(ocmb_start + start, osc, os_chunks, occ, oc_chunks);
+    while (start < end) {
+        int ocb_s = occ * jbgp.nb_oc_blocking;
+        int ocb_e = nstl::min(ocb_s + jbgp.nb_oc_blocking, jbgp.nb_oc);
+
+        int osb_s = osc * jbgp.nb_os_blocking;
+        int osb_e = nstl::min(osb_s + jbgp.nb_os_blocking, jbgp.nb_os);
+
+        for (int osb = osb_s; osb < osb_e; ++osb) {
+            int cur_os_block
+                    = nstl::min(jbgp.os - osb * jbgp.os_block, jbgp.os_block);
+            const bool is_os_tail = cur_os_block < jbgp.os_block;
+            const int cur_oc_chunk_size
+                    = nstl::min(jbgp.LDC, ocb_e * jbgp.oc_block)
+                    - ocb_s * jbgp.oc_block;
+            char *dst_reduced = (jbgp.with_sum || jbgp.acc_dt != jbgp.dst_dt
+                                                ? c_buffer_global
+                                                : dst)
+                    + get_dst_reduced_off(jbgp, dst_d, 0, osb, ocb_s);
+            const size_t os_offset = jbgp.LDC * acc_dt_size;
+            for (int ic_buf = 0; ic_buf < nthr_ic - 1; ++ic_buf) {
+                const char *c_buffer = c_buffer_global
+                        + get_dst_reduced_off(
+                                jbgp, dst_d, ic_buf + 1, osb, ocb_s);
+                for (int os = 0; os < cur_os_block; ++os) {
+                    acc_ker_->accumulate(
+                            (float *)(dst_reduced + os * os_offset),
+                            (float *)(c_buffer + os * os_offset),
+                            cur_oc_chunk_size);
+                }
+            }
+            if (are_post_ops_applicable) {
+                for (int ocb = ocb_s; ocb < ocb_e; ++ocb) {
+                    const bool is_oc_tail
+                            = (jbgp.oc - ocb * jbgp.oc_block < jbgp.oc_block);
+                    const int brg_ker_idx
+                            = brgemm_inner_product_utils::get_brg_kernel_index(
+                                    jbgp, false, false, is_os_tail, is_oc_tail,
+                                    false);
+                    maybe_tile_configure(is_amx, brg_kernel_palettes_,
+                            brg_ker_idx, prev_ker_idx);
+                    const auto brg_kernel = brg_kernels_[brg_ker_idx].get();
+                    const int os = osb * jbgp.os_block;
+                    const int oc = ocb * jbgp.oc_block;
+                    const auto ptr_bias = jbgp.with_bias
+                            ? bias + bia_dt_size * oc
+                            : nullptr;
+                    auto ptr_D = dst + get_blk_off(dst_d, jbgp.dst_dt, os, oc);
+                    auto ptr_C = (jbgp.with_sum || jbgp.acc_dt != jbgp.dst_dt
+                                                 ? c_buffer_global
+                                                 : dst)
+                            + get_dst_reduced_off(jbgp, dst_d, 0, osb, ocb);
+
+                    char *wsp_tile = is_amx ? wsp_tile_base
+                                    + ithr * jbgp.amx_buf_size_per_thread
+                                            : nullptr;
+
+                    void *scratch = is_amx
+                            ? static_cast<void *>(wsp_tile)
+                            : (jbgp.req_s8s8_compensation ? static_cast<void *>(
+                                       const_cast<int *>(&compensation[oc]))
+                                                          : nullptr);
+
+                    const brgemm_post_ops_data_t post_ops_data {
+                            static_cast<const void *>(ptr_bias),
+                            &oscales[jbgp.is_oc_scale * oc],
+                            post_ops_binary_rhs_arg_vec.data(),
+                            static_cast<size_t>(oc), 0, dst, 0, nullptr,
+                            nullptr, nullptr, true /* skip_accm */, 1, false,
+                            false, dst_scales};
+
+                    brgemm_kernel_execute_postops(brg_kernel, 0, nullptr,
+                            (void *)ptr_C, (void *)ptr_D, post_ops_data,
+                            scratch);
+                }
+            }
+        }
+        ++start;
+        nd_iterator_step(osc, os_chunks, occ, oc_chunks);
+    }
+}
 template <cpu_isa_t isa>
 status_t brgemm_inner_product_fwd_t<isa>::execute_forward(
         const exec_ctx_t &ctx) const {
@@ -157,7 +299,9 @@ status_t brgemm_inner_product_fwd_t<isa>::execute_forward(
         const size_t dst_off = get_blk_off(dst_d, jbgp.dst_dt, n, oc);
 
         const bool use_c_buffer = (jbgp.with_sum)
-                || (jbgp.use_buffer && (jbgp.nthr_ic_b == 1 || ithr_ic > 0));
+                || (jbgp.use_buffer
+                        && (jbgp.nthr_ic_b == 1 || ithr_ic > 0
+                                || jbgp.acc_dt != jbgp.dst_dt));
 
         char *c_buffer = nullptr;
         if (use_c_buffer) {
@@ -296,29 +440,30 @@ status_t brgemm_inner_product_fwd_t<isa>::execute_forward(
     const int oc_chunks = div_up(jbgp.nb_oc, jbgp.nb_oc_blocking);
     const int work_amount = oc_chunks * os_chunks;
 
-    const auto init_thr_groups
-            = [&](const int ithr, const int nthr, int &nthr_ic, int &nthr_oc_mb,
-                      int &ithr_ic, int &ithr_oc_mb) {
-                  nthr_ic = jbgp.nthr_ic_b <= nthr ? jbgp.nthr_ic_b : 1;
-                  nthr_oc_mb = nthr / nthr_ic;
-                  ithr_ic = ithr / nthr_oc_mb;
-                  ithr_oc_mb = ithr % nthr_oc_mb;
-                  if (ithr_oc_mb >= work_amount || ithr_ic >= ic_chunks
-                          || ithr >= rnd_dn(nthr, nthr_ic))
-                      return false;
-                  return true;
-              };
-
     // If work_amount == 1 we limit num_threads to 1 as parallel(1, ...) does
     // not create parallel section at all. We do not limit num_threads
     // for 1 < work_amount < dnnl_get_max_threads() case to avoid potential
     // overhead on spawning different number of OMP threads from layer to layer.
     const int num_threads
             = work_amount == 1 && jbgp.nthr_ic_b <= 1 ? 1 : jbgp.nthr;
+    bool use_local_barriers = dnnl_thr_syncable() && jbgp.nthr_ic_b > 1;
+    simple_barrier::ctx_t *barrier_ctx = use_local_barriers
+            ? scratchpad.template get<simple_barrier::ctx_t>(
+                    key_conv_wei_bia_reduction_bctx)
+            : nullptr;
+    const bool single_reduction_barrier = false;
+    if (use_local_barriers) {
+        const int num_barriers = single_reduction_barrier
+                ? 1
+                : div_up(jbgp.nthr, jbgp.nthr_ic_b);
+        for (int n = 0; n < num_barriers; n++)
+            simple_barrier::ctx_init(&barrier_ctx[n]);
+    }
+
     parallel(num_threads, [&](const int ithr, const int nthr) {
         int nthr_ic {1}, nthr_oc_mb {1}, ithr_ic {0}, ithr_oc_mb {0};
         bool ok = init_thr_groups(
-                ithr, nthr, nthr_ic, nthr_oc_mb, ithr_ic, ithr_oc_mb);
+                jbgp, ithr, nthr, nthr_ic, nthr_oc_mb, ithr_ic, ithr_oc_mb);
         if (!ok) return;
 
         int start_init {0}, end {0};
@@ -430,117 +575,26 @@ status_t brgemm_inner_product_fwd_t<isa>::execute_forward(
                     break;
             }
         }
+
+        if (jbgp.nthr_ic_b > 1 && use_local_barriers) {
+            if (single_reduction_barrier)
+                simple_barrier::barrier(barrier_ctx, jbgp.nthr);
+            else
+                simple_barrier::barrier(&barrier_ctx[ithr_oc_mb], nthr_ic);
+            maybe_reduce_and_apply_postops(jbgp, dst_d, ithr, nthr,
+                    c_buffer_global, dst, are_post_ops_applicable, bias,
+                    oscales, dst_scales, wsp_tile_base, compensation,
+                    post_ops_binary_rhs_arg_vec);
+        }
         if (is_amx) amx_tile_release();
     });
 
-    if (jbgp.nthr_ic_b > 1) {
-        assert(jbgp.use_buffer && is_f32);
-
-        const auto get_dst_reduced_off = [&](int ithr_ic, int osb, int ocb) {
-            assert(jbgp.nthr_ic_b > 1);
-            int os = osb * jbgp.os_block;
-            int oc = ocb * jbgp.oc_block;
-            const size_t dst_off = get_blk_off(dst_d, jbgp.dst_dt, os, oc);
-            if (ithr_ic == 0) return dst_off;
-            assert(ithr_ic > 0);
-            const size_t ic_buf_idx = jbgp.with_sum ? ithr_ic : ithr_ic - 1;
-            return dst_off + (ic_buf_idx * jbgp.mb * jbgp.LDC * acc_dt_size);
-        };
-
+    if (jbgp.nthr_ic_b > 1 && !use_local_barriers) {
         parallel(num_threads, [&](const int ithr, const int nthr) {
-            int nthr_ic {1}, nthr_oc_mb {1}, ithr_ic {0}, ithr_oc_mb {0};
-            bool ok = init_thr_groups(
-                    ithr, nthr, nthr_ic, nthr_oc_mb, ithr_ic, ithr_oc_mb);
-            if (!ok) return;
-
-            int ocmb_start {0}, ocmb_end {0};
-            int start {0}, end {0};
-            balance211(
-                    work_amount, nthr_oc_mb, ithr_oc_mb, ocmb_start, ocmb_end);
-            balance211(ocmb_end - ocmb_start, nthr_ic, ithr_ic, start, end);
-
-            int prev_ker_idx = -1;
-
-            int occ {0}, osc {0};
-            nd_iterator_init(
-                    ocmb_start + start, osc, os_chunks, occ, oc_chunks);
-            while (start < end) {
-                int ocb_s = occ * jbgp.nb_oc_blocking;
-                int ocb_e = nstl::min(ocb_s + jbgp.nb_oc_blocking, jbgp.nb_oc);
-
-                int osb_s = osc * jbgp.nb_os_blocking;
-                int osb_e = nstl::min(osb_s + jbgp.nb_os_blocking, jbgp.nb_os);
-
-                for (int osb = osb_s; osb < osb_e; ++osb) {
-                    int cur_os_block = nstl::min(
-                            jbgp.os - osb * jbgp.os_block, jbgp.os_block);
-                    const bool is_os_tail = cur_os_block < jbgp.os_block;
-                    const int cur_oc_chunk_size
-                            = nstl::min(jbgp.LDC, ocb_e * jbgp.oc_block)
-                            - ocb_s * jbgp.oc_block;
-                    char *dst_reduced = (jbgp.with_sum ? c_buffer_global : dst)
-                            + get_dst_reduced_off(0, osb, ocb_s);
-                    const size_t os_offset = jbgp.LDC * acc_dt_size;
-                    for (int ic_buf = 0; ic_buf < nthr_ic - 1; ++ic_buf) {
-                        const char *c_buffer = c_buffer_global
-                                + get_dst_reduced_off(ic_buf + 1, osb, ocb_s);
-                        for (int os = 0; os < cur_os_block; ++os) {
-                            acc_ker_->accumulate(
-                                    (float *)(dst_reduced + os * os_offset),
-                                    (float *)(c_buffer + os * os_offset),
-                                    cur_oc_chunk_size);
-                        }
-                    }
-                    if (are_post_ops_applicable) {
-                        for (int ocb = ocb_s; ocb < ocb_e; ++ocb) {
-                            const bool is_oc_tail
-                                    = (jbgp.oc - ocb * jbgp.oc_block
-                                            < jbgp.oc_block);
-                            const int brg_ker_idx = brgemm_inner_product_utils::
-                                    get_brg_kernel_index(jbgp, false, false,
-                                            is_os_tail, is_oc_tail, false);
-                            maybe_tile_configure(is_amx, brg_kernel_palettes_,
-                                    brg_ker_idx, prev_ker_idx);
-                            const auto brg_kernel
-                                    = brg_kernels_[brg_ker_idx].get();
-                            const int os = osb * jbgp.os_block;
-                            const int oc = ocb * jbgp.oc_block;
-                            const auto ptr_bias = jbgp.with_bias
-                                    ? bias + bia_dt_size * oc
-                                    : nullptr;
-                            auto ptr_D = dst
-                                    + get_blk_off(dst_d, jbgp.dst_dt, os, oc);
-                            auto ptr_C = (jbgp.with_sum ? c_buffer_global : dst)
-                                    + get_dst_reduced_off(0, osb, ocb);
-
-                            char *wsp_tile = is_amx ? wsp_tile_base
-                                            + ithr * jbgp.amx_buf_size_per_thread
-                                                    : nullptr;
-
-                            void *scratch = is_amx
-                                    ? static_cast<void *>(wsp_tile)
-                                    : (jbgp.req_s8s8_compensation ? static_cast<
-                                                    void *>(const_cast<int *>(
-                                               &compensation[oc]))
-                                                                  : nullptr);
-
-                            const brgemm_post_ops_data_t post_ops_data {
-                                    static_cast<const void *>(ptr_bias),
-                                    &oscales[jbgp.is_oc_scale * oc],
-                                    post_ops_binary_rhs_arg_vec.data(),
-                                    static_cast<size_t>(oc), 0, dst, 0, nullptr,
-                                    nullptr, nullptr, true /* skip_accm */, 1,
-                                    false, false, dst_scales};
-
-                            brgemm_kernel_execute_postops(brg_kernel, 0,
-                                    nullptr, (void *)ptr_C, (void *)ptr_D,
-                                    post_ops_data, scratch);
-                        }
-                    }
-                }
-                ++start;
-                nd_iterator_step(osc, os_chunks, occ, oc_chunks);
-            }
+            maybe_reduce_and_apply_postops(jbgp, dst_d, ithr, nthr,
+                    c_buffer_global, dst, are_post_ops_applicable, bias,
+                    oscales, dst_scales, wsp_tile_base, compensation,
+                    post_ops_binary_rhs_arg_vec);
         });
     }
 
diff --git a/src/cpu/x64/jit_brgemm_inner_product.hpp b/src/cpu/x64/jit_brgemm_inner_product.hpp
index ca0398b..492117e 100644
--- a/src/cpu/x64/jit_brgemm_inner_product.hpp
+++ b/src/cpu/x64/jit_brgemm_inner_product.hpp
@@ -108,11 +108,6 @@ struct brgemm_inner_product_fwd_t : public primitive_t {
                 CHECK(brgemm_desc_set_postops(
                         &brg, attr(), &dst_md_, LDD, jbgp_.bia_dt));
 
-                if (are_post_ops_applicable && jbgp_.nthr_ic_b > 1) {
-                    brgemm_attr_t brgattr;
-                    brgattr.generate_skip_accumulation = true;
-                    CHECK(brgemm_desc_set_attr(&brg, brgattr));
-                }
                 if (jbgp_.is_amx) {
                     brgemm_attr_t brgattr;
                     brgattr.max_bs = bs;
@@ -125,11 +120,17 @@ struct brgemm_inner_product_fwd_t : public primitive_t {
                     brgattr.use_interleave_stores = jbgp_.use_interleave_stores;
                     brgattr.hint_prefetching = jbgp_.hint_prefetching;
                     brgattr.fpmath_mode = attr()->fpmath_mode_;
+                    brgattr.generate_skip_accumulation
+                            = are_post_ops_applicable && jbgp_.nthr_ic_b > 1;
 
                     CHECK(brgemm_desc_set_attr(&brg, brgattr));
                     jbgp_.amx_buf_size_per_thread
                             = nstl::max(brg.get_wsp_buffer_size(),
                                     jbgp_.amx_buf_size_per_thread);
+                } else if (are_post_ops_applicable && jbgp_.nthr_ic_b > 1) {
+                    brgemm_attr_t brgattr;
+                    brgattr.generate_skip_accumulation = true;
+                    CHECK(brgemm_desc_set_attr(&brg, brgattr));
                 }
             }
 
@@ -211,6 +212,12 @@ struct brgemm_inner_product_fwd_t : public primitive_t {
 
 private:
     status_t execute_forward(const exec_ctx_t &ctx) const;
+    void maybe_reduce_and_apply_postops(const jit_brgemm_primitive_conf_t &jbgp,
+            const memory_desc_wrapper &dst_d, int ithr, int nthr,
+            char *c_buffer_global, char *dst, bool are_post_ops_applicable,
+            const char *bias, const float *oscales, const float *dst_scales,
+            char *wsp_tile_base, const int32_t *compensation,
+            std::vector<const void *> post_ops_binary_rhs_arg_vec) const;
     const pd_t *pd() const { return (const pd_t *)primitive_t::pd().get(); }
 
     std::unique_ptr<brgemm_kernel_t>
diff --git a/src/cpu/x64/jit_brgemm_inner_product_utils.cpp b/src/cpu/x64/jit_brgemm_inner_product_utils.cpp
index c82dcd3..8a4d96a 100644
--- a/src/cpu/x64/jit_brgemm_inner_product_utils.cpp
+++ b/src/cpu/x64/jit_brgemm_inner_product_utils.cpp
@@ -15,6 +15,7 @@
 *******************************************************************************/
 
 #include "cpu/x64/jit_brgemm_inner_product_utils.hpp"
+//#define BRGEMM_IP_SETUP_PARAMETERS 1
 
 namespace dnnl {
 namespace impl {
@@ -381,9 +382,14 @@ status_t init_ip_conf_fwd(jit_brgemm_primitive_conf_t &jbgp,
     //  * number of threads > 1
     //  * not a "gigantic shape" since it already has a lot of parallelism
     //      in mb and oc dimensions w/o enabling IC parallelism
-    const bool use_parallel_ic_reduction = is_f32_compute && jbgp.ic > 1024
-            && num_work_to_parallel < jbgp.nb_ic && jbgp.nthr > 1
-            && !is_gigantic_shape;
+    const bool use_parallel_ic_reduction_for_bf16 = is_amx_xf16
+            && jbgp.ic > 1024 && jbgp.oc <= 4 * 1024 && jbgp.os <= 32
+            && jbgp.nthr >= 56;
+    const bool use_parallel_ic_reduction_for_f32 = is_f32_compute
+            && jbgp.ic > 1024 && num_work_to_parallel < jbgp.nb_ic
+            && jbgp.nthr > 1 && !is_gigantic_shape;
+    const bool use_parallel_ic_reduction = use_parallel_ic_reduction_for_f32
+            || use_parallel_ic_reduction_for_bf16;
 
     // For os > 256, compute all os blocks as a single chunk when performing
     // IC reduction. Note that this condition is empirical
@@ -422,7 +428,9 @@ status_t init_ip_conf_fwd(jit_brgemm_primitive_conf_t &jbgp,
             }
         }
 
-        jbgp.nthr_ic_b = saturate(1, max_nthr_ic_b, reduce_work);
+        jbgp.nthr_ic_b = use_parallel_ic_reduction_for_bf16
+                ? 7
+                : saturate(1, max_nthr_ic_b, reduce_work);
 
         bool is_1d_oc = os_chunks == 1 && oc_chunks > 1;
         bool is_1d_os = oc_chunks == 1 && os_chunks > 1;
@@ -519,6 +527,26 @@ status_t init_ip_conf_fwd(jit_brgemm_primitive_conf_t &jbgp,
             return status::unimplemented;
     }
 
+#if defined(BRGEMM_IP_SETUP_PARAMETERS)
+    printf("IP decomposition\n\t"
+           "M(os) = %d, M_blk = %d, M_chunk = %d, M_tail = %d, num_M_chunks = "
+           "%d, LDA = %d, copy_A = %s\n\t"
+           "N = %d, N_blk = %d, N_chunk = %d, N_tail = %d, num_N_chunks = %d, "
+           "LDB = %d\n\t"
+           "K = %d, K_blk = %d, nb_ic_blocking = %d (brgemm_batch = %d), "
+           "K_tail = %d\n\t"
+           "nthr_k = %d\n",
+            (int)jbgp.os, (int)jbgp.os_block, (int)jbgp.nb_os_blocking,
+            (int)jbgp.M_tail,
+            (int)div_up(div_up(jbgp.os, jbgp.os_block), jbgp.nb_os_blocking),
+            (int)jbgp.LDA, jbgp.use_buffer_a ? "true" : "false", (int)jbgp.oc,
+            (int)jbgp.oc_block, (int)jbgp.nb_oc_blocking, (int)jbgp.N_tail,
+            (int)div_up(div_up(jbgp.oc, jbgp.oc_block), jbgp.nb_oc_blocking),
+            (int)jbgp.LDB, (int)jbgp.ic, (int)jbgp.K, (int)jbgp.nb_ic_blocking,
+            (int)jbgp.gemm_batch_size, (int)jbgp.K_tail, (int)jbgp.nthr_ic_b),
+            fflush(0);
+#endif
+
     return status::success;
 }
 
@@ -1217,7 +1245,7 @@ void init_scratchpad(memory_tracking::registrar_t &scratchpad,
         } else if (one_of(jbgp.prop_kind, forward_training, forward_inference)
                 && jbgp.nthr_ic_b > 1) {
             const bool need_extra_buffer
-                    = (jbgp.dst_dt == f32 && jbgp.with_sum);
+                    = (jbgp.dst_dt != f32 || jbgp.with_sum);
             int n_reduction_buffers = jbgp.nthr_ic_b - !need_extra_buffer;
             nelements = (size_t)n_reduction_buffers * jbgp.oc * jbgp.os;
         }
@@ -1287,6 +1315,15 @@ void init_scratchpad(memory_tracking::registrar_t &scratchpad,
         scratchpad.book<simple_barrier::ctx_t>(
                 key_conv_wei_bia_reduction_bctx, 1);
 
+    if (dnnl_thr_syncable()
+            && utils::one_of(jbgp.prop_kind, dnnl_forward_inference,
+                    dnnl_forward_training)
+            && jbgp.nthr_ic_b > 1) {
+        const int num_barriers = div_up(jbgp.nthr, jbgp.nthr_ic_b);
+        scratchpad.book<simple_barrier::ctx_t>(
+                key_conv_wei_bia_reduction_bctx, num_barriers);
+    }
+
     if (jbgp.is_amx)
         scratchpad.book(key_conv_amx_tile_buffer,
                 (size_t)jbgp.nthr * jbgp.amx_buf_size_per_thread, sizeof(char));
